---
title: "Final Report"
author: "Akshita Sure"
from: markdown+emoji
categories: [gsoc]
---

![](assets/static/logo.png){width=400 fig-align="center"}

# Abstract

This final report contains details of all of the work done during the Google Summer of Code, 2025, with respect to the nx-parallel project, a parallel backend for NetworkX that leverages joblib for running graph algorithms in parallel. I worked on switching the default Networkx configuration to use all of available cores instead of None which involved heavy documentation changes throughout. I added a timing script that utilises the `timeit` module and worked on nullifying the noise in the timings obtained. I refined the ASV benchmarks to use setup functions without including the graph creation and conversion timings. I added the `should_run` functionality which helps in determining whether a backend should run a particular algorithm. Coming to the main goal of the project, I added around 19 new embarassingly parallel algorithms to the codebase. I also revisited previously added algorithms and enhanced their performance.

I hope to make sure that this report along with my published [blogs](https://github.com/akshitasure12/networkx-blogs/tree/main/blogs) hope to serve as a key resource to help in contributions etcs.

# Background

The nx-parallel package is a backend for NetworkX designed to accelerate graph algorithms through parallel computing with joblib. Prior to the coding phase, I contributed by fixing a few utility functions, such as correcting the faulty `chunks()` function (which previously led to incorrect results due to improper passing of parameters) and refactoring `get_all_functions()`. I also proposed setting NetworkX’s default configuration to use all available cores instead of `None`

# Work done

## Adding Embarassingly parallel algorithms

The primary objective of the project was to implement seven embarrassingly parallel algorithms. By the end of the coding phase, around nineteen such graph algorithms were added to nx-parallel:

| File                  | Functions                                                                 |
|------------------------|---------------------------------------------------------------------------|
| `cluster.py`           | `triangles`, `clustering`, `average_clustering`                          |
| `link_prediction.py`   | `apply_prediction`^1^, `jaccard_coefficient`, `resource_allocation_index`, `adamic_adar_index`, `preferential_attachment`, `common_neighbor_centrality`, `cn_soundarajan_hopcroft`, `ra_index_soundarajan_hopcroft`, `within_inter_cluster` |
| `harmonic.py`          | `harmonic_centrality`                                                    |
| `neighbor_degree.py`   | `average_neighbor_degree`                                                |
| `attracting.py`        | `number_attracting_components`                                           |
| `connected.py`         | `number_connected_components`                                            |
| `strongly_connected.py`| `number_strongly_connected_components`                                   |
| `weakly_connected.py`  | `number_weakly_connected_components`                                     |
| `dag.py`               | `colliders`, `v_structures`                                              |

^1^ *A private parallel helper function that applies each internal prediction function used by the link prediction algorithms across multiple cores.*

These algorithms gave me the opportunity to explore a variety of directions and challenges in parallel graph computation. In the process, I dug into several aspects of why certain algorithms exhibit significant speedups while others do not. For example, some algorithms scale well only on sparse graphs, while their performance gains diminish on denser graphs due to the cost of serializing graph data and distributing work to multiple processes. In other cases, the rate of speedup decreases as the number of nodes increases, since the overhead of chunking tasks, pickling/unpickling, and scheduling workers outweighs the benefits of parallel execution.

I also discovered implementation-specific constraints. For instance, returning generators inside the per-chunk function caused issues, as they cannot be pickled for process communication with worker processes. Similarly, tuning parameters such as `max_chunk_size` proved important—smaller chunks worked better for certain algorithms where fine-grained load balancing mattered, while larger chunks were more efficient for others by reducing scheduling overhead.

These explorations not only deepened my understanding of parallel execution trade-offs in graph algorithms but also helped shape a set of practical guidelines for when and how parallelization in nx-parallel provides the most benefit.

## Improving the timing script (ref. [PR#114](https://github.com/networkx/nx-parallel/pull/114))

The earlier timing script sometimes produced inconsistent results. For instance, it occasionally showed speedup decreasing as the number of nodes increased, or even reported lower speedups when more cores were used—behaviors that contradict the expected trends in parallel performance.

To resolve this, I implemented a new timing script based on the timeit module. Each measurement is repeated several times, and the minimum runtime is recorded to reduce the impact of system noise. This produces results that are more stable and more representative of the true execution time.

The impact of this change can be seen below:

**Old heatmap**:

![](assets/static/clustering.png){width=100%} 

**New heatmap**:
![](assets/static/clustering_new.png){width=100%}

## Adding a `should_run` parameter (ref. [PR#123](https://github.com/networkx/nx-parallel/pull/123))

To give more control over when the parallel backend is used, a `should_run` parameter was introduced. Every algorithm is wrapped with a `default_should_run` function that checks the runtime configuration and decides whether the backend should be activated. By default, the backend is triggered only when the number of jobs utilised is greater than one, since running in parallel on a single core would not yield any benefit.

Beyond this default, other policies have also been added to handle situations where parallel execution may not be advantageous. For instance, if the measured speedup is consistently below one, the backend can be skipped in favor of the sequential version. Similarly, when the input graph is very small, the overhead may outweigh any gains, so the algorithm is forced to run sequentially or when the graph is too dense to benefit from parallel processing.

Depending on the situation, the appropriate `should_run` policy is selected and passed through the `should_run` parameter of the `_configure_if_nx_active` decorator. This design makes the backend more adaptable to different workloads and avoids wasting resources on scenarios where parallelism provides little or no benefit.

## Switching the default config to NetworkX (ref. [PR#122](https://github.com/networkx/nx-parallel/pull/122))

Previously, nx-parallel relied on Joblib’s configuration, where the default setting (n_jobs=None) restricted execution to a single core unless explicitly changed by the user. This often required additional configuration steps before any parallel speedup could be observed. 

The defaults have now been switched to use NetworkX’s configuration system instead. As part of this change, n_jobs is set to -1 by default, instructing Joblib to use all available cores automatically. This allows users to benefit from parallel execution out of the box, without needing to modify configuration settings. The documentation has been updated to reflect this change and ensure consistency with the new configuration system. Advanced users still retain the ability to override the defaults if they require finer control. 

## Implement mem-mapping

To reduce the memory overhead of passing large graphs between processes, I introduced [memmapping](https://joblib.readthedocs.io/en/latest/parallel.html#working-with-numerical-data-in-shared-memory-memmapping:~:text=shared%20memory%20(memmapping)-,%C2%B6,-By%20default%20the). The adjacency matrix is first converted to a NumPy array and dumped to disk with Joblib which allows all processes to share the same underlying file segment instead of creating separate in-memory copies. This significantly improves efficiency when working with large graphs. I applied it in `is_reachable` (ref. [PR#119](https://github.com/networkx/nx-parallel/pull/119)), but it can be extended to other algorithms as needed.

## Adding setup functions to benchmarks (ref. [PR#126](https://github.com/networkx/nx-parallel/pull/126))

Introduced a `setup` function in each benchmark class to handle graph creation and other external computations before timing the algorithm.

# Links to work done

My blogs: [Biweekly blogs](https://github.com/akshitasure12/networkx-blogs/tree/main/blogs)

## PRs opened

| PR Description                             | Link                                                         | Status|
| :------------------------------------------| :----------------------------------------------------------: | ----: |
| parallel implementation of `triangles` | [PR#106](https://github.com/networkx/nx-parallel/pull/106) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| adding a custom marker to avoid `pytest.mark.order` warning | [PR#107](https://github.com/networkx/nx-parallel/pull/107) | ![](assets/static/closed.png){width=100 fig-align="center"} |
| refactor `chunks()` to correctly use `n_jobs` |  [PR#112](https://github.com/networkx/nx-parallel/pull/112)  | ![](assets/static/merged.png){width=100 fig-align="center"}|
| improve the timing script   |  [PR#114](https://github.com/networkx/nx-parallel/pull/114)   | ![](assets/static/merged.png){width=100 fig-align="center"} |
| parallel implementation of `number_` algorithms|  [PR#117](https://github.com/networkx/nx-parallel/pull/117)  | ![](assets/static/merged.png){width=100 fig-align="center"} |
| modify `is_reachable()` to use mem-mapping approach | [PR#119](https://github.com/networkx/nx-parallel/pull/119)|![](assets/static/merged.png){width=100 fig-align="center"} |
| optimise `is_reachable()` in NetworkX | [PR#8112](https://github.com/networkx/networkx/pull/8112) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| remove test order dependency via context managers| [PR#120](https://github.com/networkx/nx-parallel/pull/120) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| make `n_jobs=-1` as defualt | [PR#122](https://github.com/networkx/nx-parallel/pull/122) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| add `should_run` functionality | [PR#123](https://github.com/networkx/nx-parallel/pull/123) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| parallel implementation of `harmonic_centrality` | [PR#124](https://github.com/networkx/nx-parallel/pull/124) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| optimise harmonic centrality | [PR#8158](https://github.com/networkx/networkx/pull/8158) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| refactor ASV benchmarks with setup functions | [PR#126](https://github.com/networkx/nx-parallel/pull/126) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| parallel implementation of `link_prediction` algorithms | [PR#127](https://github.com/networkx/nx-parallel/pull/127) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| refactor `test_get_functions_with_get_chunks` | [PR#128](https://github.com/networkx/nx-parallel/pull/128) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| update `test_get_chunks` for new algorithms | [PR#129](https://github.com/networkx/nx-parallel/pull/129) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| parallel implementation of `clustering` and `average_clustering` | [PR#130](https://github.com/networkx/nx-parallel/pull/130) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| uses `pytest.raises` as context | [PR#8170](https://github.com/networkx/networkx/pull/8170) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| parallel implementation of `average_neighbor_degree` | [PR#132](https://github.com/networkx/nx-parallel/pull/132) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| move `assign_algorithms` outside `BackendInterface` class | [PR#133](https://github.com/networkx/nx-parallel/pull/133) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| parallel implementation of `v_structures` and `colliders` | [PR#134](https://github.com/networkx/nx-parallel/pull/134) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| simplify node selection using `nbunch_iter()` | [PR#135](https://github.com/networkx/nx-parallel/pull/135) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| post merge refinement | [PR#138](https://github.com/networkx/nx-parallel/pull/138) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| add `should_run` for when `nodes=None` | [PR#141](https://github.com/networkx/nx-parallel/pull/141) | ![](assets/static/open.png){width=100 fig-align="center"} |
| clean up | [PR#142](https://github.com/networkx/nx-parallel/pull/142) | ![](assets/static/open.png){width=100 fig-align="center"} |

## Issues Raised

| Issue Description                                                          | Link                                                         | Status|
| :--------------------------------------------------------------------------| :----------------------------------------------------------: | ----: |
| `get_all_functions()` is not returning args and kwargs of the functions | [PR#94](https://github.com/networkx/nx-parallel/issues/94) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| Incorrect passing of `num_in_chunk` as `n_jobs` in `chunks()` | [PR#110](https://github.com/networkx/nx-parallel/issues/110) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| set `n_jobs=-1` as default | [PR#111](https://github.com/networkx/nx-parallel/issues/111) | ![](assets/static/merged.png){width=100 fig-align="center"} |
| set `should_run=False` unless `nodes` is `None` | [PR#110](https://github.com/networkx/nx-parallel/issues/137) | ![](assets/static/open.png){width=100 fig-align="center"} |

# Learnings and Challenges

Looking back, I can see just how far I’ve come over the course of my GSoC journey. One of the biggest lessons I’ve learned is the true value of documentation—not just as a formality, but as a way to communicate ideas clearly and leave a trail of insights for others (and even for myself in the future). I’ve developed the habit of recording my findings as I go, and over time I’ve become more mindful of keeping things concise, relevant, and easy to follow.

This experience also made me more curious by nature-- I now find myself constantly wondering why something works the way it does and digging deeper to understand it better. Along with that curiosity, I learned the importance of asking questions and not being afraid to seek clarification from mentors. Collaborating in an open-source setting also taught me how valuable community discussions are; they not only improved code quality but also broadened my perspective on different approaches to solving a particular problem.

I also learned the importance of testing and benchmarking-- identifying bottlenecks, and validating improvements in a systematic manner. Through this, I gained a stronger appreciation for writing maintainable code that others can build on, not just code that “works.”

One of the main challenges I faced was maintaining a balance between staying focused on the defined tasks and resisting the temptation to dive too deeply into side explorations. It’s easy to get carried away chasing interesting rabbit holes, but I had to remind myself to stay grounded in the project’s goals while still leaving room for curiosity and experimentation. Finding that balance wasn’t always easy, but it taught me a lot about prioritization.

# Conclusion

The GSoC 2025 experience has truly been one of a kind. It has shaped me into a more well-rounded developer, going far beyond just “writing code.” I learned the importance of documentation, testing, benchmarking, collaboration, and community-driven development-- all of which are just as vital as code itself. In many ways, I feel I’ve experienced exactly what GSoC is intended to inspire: a genuine love for contributing to open source and a deeper appreciation for the impact of collective effort.

When I look back at my original proposal, my plan was to implement 7 parallel graph algorithms. By the end of the summer, however, not only did I added those many algorithms but also worked across many other aspects of the project. This growth was possible because I became increasingly comfortable with the codebase, and with that came the confidence to explore, experiment, and contribute in new directions.

Looking ahead, I plan to continue being an active part of the NetworkX and nx-parallel communities. I want to keep fixing bugs, improving features, and helping with new ideas that push the project forward. More importantly, I look forward to continuing the habit of learning, sharing knowledge, and contributing back to open source-- because this journey has shown me that the real reward lies in building together.

# Achnowledgements

I would like to sincerely thank my mentors, Dan Schult and Aditi Juneja, for their invaluable guidance and support throughout my GSoC journey. Their thoughtful reviews not only helped me refine my work but also encouraged me to pay attention to the smallest details. The mid-term review was especially insightful, as it helped me recognize my strengths while also highlighting areas for growth—motivating me to continually improve.

I am deeply grateful for their time, patience, and encouragement. I would also like to thank Google Summer of Code for giving me the opportunity to contribute to such an inspiring open-source community and to learn in such a collaborative environment.


