[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Akshita Sure",
    "section": "",
    "text": "All GSoC work, including daily updates and blog posts, can be found at https://github.com/akshitasure12/networkx-blogs!\n\n\n© 2025 Akshita Sure"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Final Report",
    "section": "",
    "text": "Mentors: Dan Schult, Aditi Juneja  Timeline: May 2025 - August 2025\n© 2025 Akshita Sure"
  },
  {
    "objectID": "index.html#adding-embarassingly-parallel-algorithms",
    "href": "index.html#adding-embarassingly-parallel-algorithms",
    "title": "Final Report",
    "section": "Adding Embarassingly parallel algorithms",
    "text": "Adding Embarassingly parallel algorithms\nThe primary objective of the project was to implement seven embarrassingly parallel algorithms. By the end of the coding phase, around nineteen such graph algorithms were added to nx-parallel:\n\n\n\n\n\n\n\nFile\nFunctions\n\n\n\n\ncluster.py\ntriangles, clustering, average_clustering\n\n\nlink_prediction.py\napply_prediction1, jaccard_coefficient, resource_allocation_index, adamic_adar_index, preferential_attachment, common_neighbor_centrality, cn_soundarajan_hopcroft, ra_index_soundarajan_hopcroft, within_inter_cluster\n\n\nharmonic.py\nharmonic_centrality\n\n\nneighbor_degree.py\naverage_neighbor_degree\n\n\nattracting.py\nnumber_attracting_components\n\n\nconnected.py\nnumber_connected_components\n\n\nstrongly_connected.py\nnumber_strongly_connected_components\n\n\nweakly_connected.py\nnumber_weakly_connected_components\n\n\ndag.py\ncolliders, v_structures\n\n\n\n1 A private parallel helper function that applies each internal prediction function used by the link prediction algorithms across multiple cores.\nWorking with these algorithms gave me the chance to explore a wide range of challenges in parallel graph computation. Along the way, I gained insight into why some algorithms achieve significant speedups while others do not. For example, certain algorithms scale well on sparse graphs but lose their performance advantage on denser graphs due to the overhead of serializing graph data and distributing work across multiple processes. In other cases, speedup slows down as the number of nodes increases because the overhead of chunking tasks, pickling/unpickling, and scheduling workers grows.\nI also encountered implementation-specific constraints. Returning generators from within per-chunk functions, for instance, caused issues since generators cannot be pickled for communication between worker processes. Similarly, tuning parameters like max_chunk_size proved crucial– smaller chunks benefited algorithms needing fine-grained load balancing, while larger chunks improved efficiency by reducing scheduling overhead.\nThese explorations not only deepened my understanding of parallel execution trade-offs in graph algorithms but also helped shape a set of practical guidelines for when and how parallelization in nx-parallel provides the most benefit."
  },
  {
    "objectID": "index.html#improving-the-timing-script-ref.-pr114",
    "href": "index.html#improving-the-timing-script-ref.-pr114",
    "title": "Final Report",
    "section": "Improving the timing script (ref. PR#114)",
    "text": "Improving the timing script (ref. PR#114)\nThe earlier timing script produced inconsistent results, sometimes showing decreasing speedups with more nodes or cores. I replaced it with a timeit-based script that repeats measurements and records the minimum runtime, yielding more stable and accurate results. The impact of this change can be seen below:\nOld heatmap:\n\nNew heatmap:"
  },
  {
    "objectID": "index.html#adding-a-should_run-parameter-ref.-pr123",
    "href": "index.html#adding-a-should_run-parameter-ref.-pr123",
    "title": "Final Report",
    "section": "Adding a should_run parameter (ref. PR#123)",
    "text": "Adding a should_run parameter (ref. PR#123)\nA should_run parameter was added to control when the parallel backend is used. By default, it activates only when multiple jobs are available, avoiding parallel execution on a single core. Other policies handle cases where parallelism is ineffective– for example, when speedup is consistently low, the graph is very small, or it’s too dense. The appropriate policy is passed via the _configure_if_nx_active decorator, making the backend adaptable and preventing wasted resources in scenarios where parallelism provides little or no benefit."
  },
  {
    "objectID": "index.html#switching-the-default-config-to-networkx-ref.-pr122",
    "href": "index.html#switching-the-default-config-to-networkx-ref.-pr122",
    "title": "Final Report",
    "section": "Switching the default config to NetworkX (ref. PR#122)",
    "text": "Switching the default config to NetworkX (ref. PR#122)\nPreviously, nx-parallel used Joblib’s default (n_jobs=None) and ran on a single core unless explicitly changed. I switched the defaults to NetworkX’s config and set n_jobs = -1, so the config system now uses all available cores automatically. Hence, the change required a thorough update of the documentation to ensure it accurately reflects the new configuration."
  },
  {
    "objectID": "index.html#implement-mem-mapping",
    "href": "index.html#implement-mem-mapping",
    "title": "Final Report",
    "section": "Implement mem-mapping",
    "text": "Implement mem-mapping\nTo reduce the memory overhead of passing large graphs between processes, I introduced memmapping for algorithms where the adjacency matrix is first converted to a NumPy array and stored on disk using Joblib. This allows all processes to share the same underlying file segment instead of creating separate in-memory copies, significantly improving efficiency for large graphs. I applied this approach to is_reachable (ref. PR#119), but it can be extended to other algorithms as needed."
  },
  {
    "objectID": "index.html#adding-setup-functions-to-benchmarks-ref.-pr126",
    "href": "index.html#adding-setup-functions-to-benchmarks-ref.-pr126",
    "title": "Final Report",
    "section": "Adding setup functions to benchmarks (ref. PR#126)",
    "text": "Adding setup functions to benchmarks (ref. PR#126)\nIntroduced a setup function in each benchmark class to handle graph creation and other external computations before timing the algorithm."
  },
  {
    "objectID": "index.html#prs-opened",
    "href": "index.html#prs-opened",
    "title": "Final Report",
    "section": "PRs opened",
    "text": "PRs opened\nThe table below lists all the PRs I opened during the GSoC coding phase in both the nx-parallel and NetworkX repositories, along with their merge status.\n\n\n\nPR\nLink\nStatus\n\n\n\n\nparallel implementation of triangles\nPR#106\n\n\n\nadding a custom marker to avoid pytest.mark.order warning\nPR#107\n\n\n\nrefactor chunks() to correctly use n_jobs\nPR#112\n\n\n\nimprove the timing script\nPR#114\n\n\n\nparallel implementation of number_ algorithms\nPR#117\n\n\n\nmodify is_reachable() to use mem-mapping approach\nPR#119\n\n\n\noptimise is_reachable() in NetworkX\nPR#8112\n\n\n\nremove test order dependency via context managers\nPR#120\n\n\n\nmake n_jobs=-1 as defualt\nPR#122\n\n\n\nadd should_run functionality\nPR#123\n\n\n\nparallel implementation of harmonic_centrality\nPR#124\n\n\n\noptimise harmonic centrality\nPR#8158\n\n\n\nrefactor ASV benchmarks with setup functions\nPR#126\n\n\n\nparallel implementation of link_prediction algorithms\nPR#127\n\n\n\nrefactor test_get_functions_with_get_chunks\nPR#128\n\n\n\nupdate test_get_chunks for new algorithms\nPR#129\n\n\n\nparallel implementation of clustering and average_clustering\nPR#130\n\n\n\nuses pytest.raises as context\nPR#8170\n\n\n\nparallel implementation of average_neighbor_degree\nPR#132\n\n\n\nmove assign_algorithms outside BackendInterface class\nPR#133\n\n\n\nparallel implementation of v_structures and colliders\nPR#134\n\n\n\nsimplify node selection using nbunch_iter()\nPR#135\n\n\n\npost merge refinement\nPR#138\n\n\n\nadd should_run for when nodes=None\nPR#141\n\n\n\nclean up\nPR#142"
  },
  {
    "objectID": "index.html#issues-raised",
    "href": "index.html#issues-raised",
    "title": "Final Report",
    "section": "Issues Raised",
    "text": "Issues Raised\nThe table below summarizes the key issues identified and addressed during the GSoC coding phase in both the nx-parallel and NetworkX repositories, along with their current status.\n\n\n\nIssue Description\nLink\nStatus\n\n\n\n\nget_all_functions() is not returning args and kwargs of the functions\nPR#94\n\n\n\nIncorrect passing of num_in_chunk as n_jobs in chunks()\nPR#110\n\n\n\nset n_jobs=-1 as default\nPR#111\n\n\n\nset should_run=False unless nodes is None\nPR#110"
  }
]