[
  {
    "objectID": "index.html#adding-embarassingly-parallel-algorithms",
    "href": "index.html#adding-embarassingly-parallel-algorithms",
    "title": "Final Report",
    "section": "Adding Embarassingly parallel algorithms",
    "text": "Adding Embarassingly parallel algorithms\nThe primary objective of the project was to implement seven embarrassingly parallel algorithms. By the end of the coding phase, around nineteen such graph algorithms were added to nx-parallel:\n\n\n\n\n\n\n\nFile\nFunctions\n\n\n\n\ncluster.py\ntriangles, clustering, average_clustering\n\n\nlink_prediction.py\napply_prediction1, jaccard_coefficient, resource_allocation_index, adamic_adar_index, preferential_attachment, common_neighbor_centrality, cn_soundarajan_hopcroft, ra_index_soundarajan_hopcroft, within_inter_cluster\n\n\nharmonic.py\nharmonic_centrality\n\n\nneighbor_degree.py\naverage_neighbor_degree\n\n\nattracting.py\nnumber_attracting_components\n\n\nconnected.py\nnumber_connected_components\n\n\nstrongly_connected.py\nnumber_strongly_connected_components\n\n\nweakly_connected.py\nnumber_weakly_connected_components\n\n\ndag.py\ncolliders, v_structures\n\n\n\n1 A private parallel helper function that applies each internal prediction function used by the link prediction algorithms across multiple cores.\nThese algorithms gave me the opportunity to explore a variety of directions and challenges in parallel graph computation. In the process, I dug into several aspects of why certain algorithms exhibit significant speedups while others do not. For example, some algorithms scale well only on sparse graphs, while their performance gains diminish on denser graphs due to the cost of serializing graph data and distributing work to multiple processes. In other cases, the rate of speedup decreases as the number of nodes increases, since the overhead of chunking tasks, pickling/unpickling, and scheduling workers outweighs the benefits of parallel execution.\nI also discovered implementation-specific constraints. For instance, returning generators inside the per-chunk function caused issues, as they cannot be pickled for process communication with worker processes. Similarly, tuning parameters such as max_chunk_size proved important—smaller chunks worked better for certain algorithms where fine-grained load balancing mattered, while larger chunks were more efficient for others by reducing scheduling overhead.\nThese explorations not only deepened my understanding of parallel execution trade-offs in graph algorithms but also helped shape a set of practical guidelines for when and how parallelization in nx-parallel provides the most benefit."
  },
  {
    "objectID": "index.html#improving-the-timing-script-ref.-pr114",
    "href": "index.html#improving-the-timing-script-ref.-pr114",
    "title": "Final Report",
    "section": "Improving the timing script (ref. PR#114)",
    "text": "Improving the timing script (ref. PR#114)\nThe earlier timing script sometimes produced inconsistent results. For instance, it occasionally showed speedup decreasing as the number of nodes increased, or even reported lower speedups when more cores were used—behaviors that contradict the expected trends in parallel performance.\nTo resolve this, I implemented a new timing script based on the timeit module. Each measurement is repeated several times, and the minimum runtime is recorded to reduce the impact of system noise. This produces results that are more stable and more representative of the true execution time.\nThe impact of this change can be seen below:\nOld heatmap:\n\nNew heatmap:"
  },
  {
    "objectID": "index.html#adding-a-should_run-parameter-ref.-pr123",
    "href": "index.html#adding-a-should_run-parameter-ref.-pr123",
    "title": "Final Report",
    "section": "Adding a should_run parameter (ref. PR#123)",
    "text": "Adding a should_run parameter (ref. PR#123)\nTo give more control over when the parallel backend is used, a should_run parameter was introduced. Every algorithm is wrapped with a default_should_run function that checks the runtime configuration and decides whether the backend should be activated. By default, the backend is triggered only when the number of jobs utilised is greater than one, since running in parallel on a single core would not yield any benefit.\nBeyond this default, other policies have also been added to handle situations where parallel execution may not be advantageous. For instance, if the measured speedup is consistently below one, the backend can be skipped in favor of the sequential version. Similarly, when the input graph is very small, the overhead may outweigh any gains, so the algorithm is forced to run sequentially or when the graph is too dense to benefit from parallel processing.\nDepending on the situation, the appropriate should_run policy is selected and passed through the should_run parameter of the _configure_if_nx_active decorator. This design makes the backend more adaptable to different workloads and avoids wasting resources on scenarios where parallelism provides little or no benefit."
  },
  {
    "objectID": "index.html#switching-the-default-config-to-networkx-ref.-pr122",
    "href": "index.html#switching-the-default-config-to-networkx-ref.-pr122",
    "title": "Final Report",
    "section": "Switching the default config to NetworkX (ref. PR#122)",
    "text": "Switching the default config to NetworkX (ref. PR#122)\nPreviously, nx-parallel relied on Joblib’s configuration, where the default setting (n_jobs=None) restricted execution to a single core unless explicitly changed by the user. This often required additional configuration steps before any parallel speedup could be observed.\nThe defaults have now been switched to use NetworkX’s configuration system instead. As part of this change, n_jobs is set to -1 by default, instructing Joblib to use all available cores automatically. This allows users to benefit from parallel execution out of the box, without needing to modify configuration settings. The documentation has been updated to reflect this change and ensure consistency with the new configuration system. Advanced users still retain the ability to override the defaults if they require finer control."
  },
  {
    "objectID": "index.html#implement-mem-mapping",
    "href": "index.html#implement-mem-mapping",
    "title": "Final Report",
    "section": "Implement mem-mapping",
    "text": "Implement mem-mapping\nTo reduce the memory overhead of passing large graphs between processes, I introduced memmapping. The adjacency matrix is first converted to a NumPy array and dumped to disk with Joblib which allows all processes to share the same underlying file segment instead of creating separate in-memory copies. This significantly improves efficiency when working with large graphs. I applied it in is_reachable (ref. PR#119), but it can be extended to other algorithms as needed."
  },
  {
    "objectID": "index.html#adding-setup-functions-to-benchmarks-ref.-pr126",
    "href": "index.html#adding-setup-functions-to-benchmarks-ref.-pr126",
    "title": "Final Report",
    "section": "Adding setup functions to benchmarks (ref. PR#126)",
    "text": "Adding setup functions to benchmarks (ref. PR#126)\nIntroduced a setup function in each benchmark class to handle graph creation and other external computations before timing the algorithm."
  },
  {
    "objectID": "index.html#prs-opened",
    "href": "index.html#prs-opened",
    "title": "Final Report",
    "section": "PRs opened",
    "text": "PRs opened\n\n\n\nPR Description\nLink\nStatus\n\n\n\n\nparallel implementation of triangles\nPR#106\n\n\n\nadding a custom marker to avoid pytest.mark.order warning\nPR#107\n\n\n\nrefactor chunks() to correctly use n_jobs\nPR#112\n\n\n\nimprove the timing script\nPR#114\n\n\n\nparallel implementation of number_ algorithms\nPR#117\n\n\n\nmodify is_reachable() to use mem-mapping approach\nPR#119\n\n\n\noptimise is_reachable() in NetworkX\nPR#8112\n\n\n\nremove test order dependency via context managers\nPR#120\n\n\n\nmake n_jobs=-1 as defualt\nPR#122\n\n\n\nadd should_run functionality\nPR#123\n\n\n\nparallel implementation of harmonic_centrality\nPR#124\n\n\n\noptimise harmonic centrality\nPR#8158\n\n\n\nrefactor ASV benchmarks with setup functions\nPR#126\n\n\n\nparallel implementation of link_prediction algorithms\nPR#127\n\n\n\nrefactor test_get_functions_with_get_chunks\nPR#128\n\n\n\nupdate test_get_chunks for new algorithms\nPR#129\n\n\n\nparallel implementation of clustering and average_clustering\nPR#130\n\n\n\nuses pytest.raises as context\nPR#8170\n\n\n\nparallel implementation of average_neighbor_degree\nPR#132\n\n\n\nmove assign_algorithms outside BackendInterface class\nPR#133\n\n\n\nparallel implementation of v_structures and colliders\nPR#134\n\n\n\nsimplify node selection using nbunch_iter()\nPR#135\n\n\n\npost merge refinement\nPR#138\n\n\n\nadd should_run for when nodes=None\nPR#141\n\n\n\nclean up\nPR#142"
  },
  {
    "objectID": "index.html#issues-raised",
    "href": "index.html#issues-raised",
    "title": "Final Report",
    "section": "Issues Raised",
    "text": "Issues Raised\n\n\n\nIssue Description\nLink\nStatus\n\n\n\n\nget_all_functions() is not returning args and kwargs of the functions\nPR#94\n\n\n\nIncorrect passing of num_in_chunk as n_jobs in chunks()\nPR#110\n\n\n\nset n_jobs=-1 as default\nPR#111\n\n\n\nset should_run=False unless nodes is None\nPR#110"
  }
]